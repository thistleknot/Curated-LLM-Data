Data:

	wikitext-v3
	alpaca 52k
	Dolly 15k
	https://github.com/databrickslabs/dolly/tree/master/data
	newsgroup articles
	regex statements
	programming examples (w3schools)
		use condorcet voting to track which examples should be tracked
		
		just python
		
		machine learning
	merck manual medical diagnosis information
		use condorcet voting to track which examples should be tracked
		Start with choice examples
	arxiv abstracts
	investopedia
	quotes
		ex.
			https://graciousquotes.com/carl-jung/
		brainyquote.com
		www.goodreads.com/quotes
	lyrics to popular songs (top 100 billboard per year)
		genius
	front page news stories
		newspaperarchive.com
		
		use a similar concept on top voted reddit posts from select communities
		e.x.
			news
			askphilosophy
			scrape top level comments and descend 3 deep?
			academic biblical
	ted talk abstracts
	book summaries (reviews?) (ideally would be reports, essays, exposes' rather than 'this book was good or bad') to NY Times best selling non fiction
	Summaries of popular self help guides
		May be redundant with wiki	
	interesting fact a day sources
		condorcet voting to track which facts are most pertinent
	Peer reviewed findings (abstracts)
	lessthanwrong choice articles
	Philosophy
		IEP data
			for ex.
		SEP data
			https://plato.stanford.edu
	Synopsis of Oscar winner movies
		Redundant with wiki?
	Important historical events

Quantize as a knowledge graph for dense passage retrieval

Generative Questions (Text to question models)

The more comprehensive the prompt's crossover is with other idea's and concepts, the faster the model will generalize across large domains of knowledge because it will make connections between disparate nodes.

So try to pick concepts that are not correlated with other as well as those that are.  I'd say a nice 50/50 mix.  Imagine markowitz profile, the goal is find the best fit (regression mean fit) with minimimal error (risk).
